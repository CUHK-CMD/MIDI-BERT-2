{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b68ae3a-cabf-4cee-b69e-49ff750b113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy\n",
    "from miditoolkit.midi import parser as mid_parser  \n",
    "from miditoolkit.midi import containers as ct\n",
    "from numpy import array, linspace\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from matplotlib.pyplot import plot\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43dd2494-712a-4164-bd29-c4b8c4c88aee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Road map\\n----[requirements]\\n0. install requirement\\n    -- pip install -r requirements.txt\\n\\n----[prepare training data]\\n1. turn training data into (-1,512,4)  \\n    : 512 = input_size, \\n    : 4= size of CP token (includingBar (0=new,1=continue,2=pad),Position,Pitch,Duration)\\n    ** DO NOT concatenate all songs tgt, pad every song and make it divisible by 512\\n    ** pad it with (2,16,86,64)\\n    e.g. [ [song1-0:511],[song1-512:520+pad],[song2-0:511],[song2-512-530+pad].... ]\\n    \\n2. Prepare answer dataset \\n    : (0 = padding, 1 = keep, 2 = discard)\\n* save in .npy format\\n* split in 3 different groups\\n    dataroot= ~/data/CP/\\n    -custom_reduction_train.npy , custom_reduction_train_ans.npy\\n    -custom_reduction_valid.npy , custom_reduction_valid_ans.npy\\n    -custom_reduction_test.npy  , custom_reduction_test_ans.npy\\n\\n    \\n----[start fine tuning]\\n1. cd to ./MidiBERT/CP\\n2. python3 finetune.py --task=custom  --ckpt xxxxx (default='result/finetune/melody_default/model_best.ckpt')\\n* gpu with --cuda_devices 0 (+1)\\n\\n----[eval]\\n1. cd to ./MidiBERT/CP\\n2. python3 eval.py --task=reduction\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''Road map\n",
    "----[requirements]\n",
    "0. install requirement\n",
    "    -- pip install -r requirements.txt\n",
    "\n",
    "----[prepare training data]\n",
    "1. turn training data into (-1,512,4)  \n",
    "    : 512 = input_size, \n",
    "    : 4= size of CP token (includingBar (0=new,1=continue,2=pad),Position,Pitch,Duration)\n",
    "    ** DO NOT concatenate all songs tgt, pad every song and make it divisible by 512\n",
    "    ** pad it with (2,16,86,64)\n",
    "    e.g. [ [song1-0:511],[song1-512:520+pad],[song2-0:511],[song2-512-530+pad].... ]\n",
    "    \n",
    "2. Prepare answer dataset \n",
    "    : (0 = padding, 1 = keep, 2 = discard)\n",
    "* save in .npy format\n",
    "* split in 3 different groups\n",
    "    dataroot= ~/data/CP/\n",
    "    -custom_reduction_train.npy , custom_reduction_train_ans.npy\n",
    "    -custom_reduction_valid.npy , custom_reduction_valid_ans.npy\n",
    "    -custom_reduction_test.npy  , custom_reduction_test_ans.npy\n",
    "\n",
    "    \n",
    "----[start fine tuning]\n",
    "1. cd to ./MidiBERT/CP\n",
    "2. python3 finetune.py --task=reduction  --ckpt xxxxx (default='result/finetune/pretrain_model.ckpt')\n",
    "* gpu with --cuda_devices 0 (+1)\n",
    "\n",
    "----[eval]\n",
    "1. cd to ./MidiBERT/CP\n",
    "2. python3 eval.py --task=reduction\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea1673ff-6a38-4dca-af05-1d4adf991963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1c19cd-be01-47b5-bf3b-1eb7a5d49f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 512, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training data after tokenization\n",
    "a=np.load('./data/CP/custom.npy') # after tokenization\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e05cb6e-5d2f-4f9b-9a9e-07e29f475c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrsponding ans\n",
    "fake_ans= np.array([[random.randint(1,2) for _ in range(512)] for __ in range(4)])  #random generate\n",
    "fake_ans[3][206:]=0 # random padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ddf7f8-5613-40d8-901f-493be9f9a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test,train,valid split\n",
    "#assume a[0],a[1],a[2:] are three different songs\n",
    "testX,testY = a[0].reshape(-1,512,4),fake_ans[0].reshape(-1,512)\n",
    "validX,validY = a[1].reshape(-1,512,4),fake_ans[1].reshape(-1,512)\n",
    "trainX,trainY = a[2:].reshape(-1,512,4),fake_ans[2:].reshape(-1,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73287f6c-b6ed-408e-95bb-c4a3ebaa3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save training data\n",
    "np.save('./data/CP/custom_reduction_train.npy',trainX)\n",
    "np.save('./data/CP/custom_reduction_train_ans.npy',trainY)\n",
    "np.save('./data/CP/custom_reduction_valid.npy',validX)\n",
    "np.save('./data/CP/custom_reduction_valid_ans.npy',validY)\n",
    "np.save('./data/CP/custom_reduction_test.npy',testX)\n",
    "np.save('./data/CP/custom_reduction_test_ans.npy',testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56fd9d30-f548-4b81-8019-c4ada9bb81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e7bd3b1-5223-477d-acfb-efbdbb83cd9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-bee888e86011>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mload_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_model_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bert_model_path' is not defined"
     ]
    }
   ],
   "source": [
    "load_dict = torch.load(bert_model_path)\n",
    "print(load_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b5b6b-7273-4baa-88d4-81a01fbda0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start fine tuning\n",
    "! python ./MidiBERT/CP/finetune.py --task=reduction  --cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f273eeab-7435-46e3-94ce-dd72fc9891c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
